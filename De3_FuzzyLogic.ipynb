{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import plyj.parser as plyj\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para tokenizar un archivo\n",
    "def tokenizar_archivo(archivo):\n",
    "    with open(archivo, 'r') as file:\n",
    "        codigo = file.read()\n",
    "        tokens = word_tokenize(codigo)\n",
    "    return tokens\n",
    "\n",
    "# Función para parsear el código y contar características\n",
    "def parsear_y_contar_caracteristicas(codigo):\n",
    "    parser = plyj.Parser()\n",
    "    tree = parser.parse_string(codigo)\n",
    "    tokens = [str(token).strip() for token in tree.iter_tokens()]\n",
    "    features = {token: tokens.count(token) for token in set(tokens)}\n",
    "    return features\n",
    "\n",
    "# Carpeta donde se encuentran los códigos\n",
    "carpeta_codigos = \".\\\\conplag\\\\programs\"\n",
    "\n",
    "# Leer todos los códigos en la carpeta\n",
    "codigos_tokenizados = {}  # Usamos un diccionario para almacenar los tokens por nombre de archivo\n",
    "for filename in os.listdir(carpeta_codigos):\n",
    "    filepath = os.path.join(carpeta_codigos, filename)\n",
    "    if os.path.isfile(filepath):\n",
    "        tokens = tokenizar_archivo(filepath)\n",
    "        codigos_tokenizados[filename] = tokens\n",
    "\n",
    "# Mostrar los tokens por nombre de archivo\n",
    "for nombre_archivo, tokens in codigos_tokenizados.items():\n",
    "    print(f\"Tokens del archivo '{nombre_archivo}':\\n{tokens}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Convertir todos los códigos tokenizados a texto plano para TF-IDF y Markovify\n",
    "codigos_planos = [' '.join(codigo) for codigo in codigos_tokenizados.values()]\n",
    "\n",
    "# Construir un vectorizador TF-IDF\n",
    "vectorizador_tfidf = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizador_tfidf.fit_transform(codigos_planos)\n",
    "\n",
    "# Función para calcular la similitud de coseno entre dos vectores de frecuencia de términos (TF-IDF)\n",
    "def similitud_coseno(vector1, vector2):\n",
    "    return cosine_similarity(vector1.reshape(1, -1), vector2.reshape(1, -1))[0][0]\n",
    "\n",
    "# Función para calcular la similitud de Jaccard entre dos conjuntos de tokens\n",
    "def similitud_jaccard(tokens1, tokens2):\n",
    "    set1 = set(tokens1)\n",
    "    set2 = set(tokens2)\n",
    "    return len(set1.intersection(set2)) / len(set1.union(set2))\n",
    "\n",
    "data = []\n",
    "\n",
    "for nombre_archivo1, codigo_tokenizado1 in codigos_tokenizados.items():\n",
    "    for nombre_archivo2, codigo_tokenizado2 in codigos_tokenizados.items():\n",
    "        if nombre_archivo1 != nombre_archivo2:  \n",
    "            sim_jaccard = similitud_jaccard(codigo_tokenizado1, codigo_tokenizado2)\n",
    "            if sim_jaccard >= 0.5:\n",
    "                codigo_plano1 = ' '.join(codigo_tokenizado1)\n",
    "                codigo_plano2 = ' '.join(codigo_tokenizado2)\n",
    "                tfidf_codigo1 = vectorizador_tfidf.transform([codigo_plano1])\n",
    "                tfidf_codigo2 = vectorizador_tfidf.transform([codigo_plano2])\n",
    "                sim_coseno = similitud_coseno(tfidf_codigo1, tfidf_codigo2)\n",
    "                # Agregar nombres de archivos a la lista data\n",
    "                data.append([nombre_archivo1, nombre_archivo2, sim_jaccard, sim_coseno])\n",
    "                print(f\"Comparando código '{nombre_archivo1}' con código '{nombre_archivo2}':\")\n",
    "                print(f\"Similitud de coseno: {round(sim_coseno, 4)}\")\n",
    "                print(f\"Similitud de Jaccard: {round(sim_jaccard, 4)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para determinar el grado de pertenencia al plagio\n",
    "def grado_pertenencia(sim_jaccard, sim_coseno):\n",
    "    # Establecer las reglas difusas\n",
    "    if sim_jaccard >= 0.5 and sim_coseno >= 0.5:\n",
    "        return \"------ ALTA\"\n",
    "    else:\n",
    "        return None  # Retornar None si el grado de pertenencia es menor a 0.5 en alguna similitud\n",
    "\n",
    "for nombre_archivo1, nombre_archivo2, sim_jaccard, sim_coseno in data:\n",
    "    # Computar el grado de pertenencia al plagio\n",
    "    pertenencia = grado_pertenencia(sim_jaccard, sim_coseno)\n",
    "    \n",
    "    # Imprimir el grado de pertenencia al plagio junto con los nombres de los archivos\n",
    "    if pertenencia is not None:  # Imprimir solo si el grado de pertenencia no es None\n",
    "        print(f\"Comparando código '{nombre_archivo1}' con código '{nombre_archivo2}':\")\n",
    "        print(\"Similitud Jaccard:\", round(sim_jaccard, 4))\n",
    "        print(\"Similitud Coseno:\", round(sim_coseno, 4))\n",
    "        print(\"Grado de pertenencia al plagio:\", pertenencia)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
